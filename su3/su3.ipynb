{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fon-TheManHimself: \n",
    "\n",
    "NameError                                 Traceback (most recent call last)\n",
    "Cell In[9], line 29\n",
    "     26 epochs = 1000\n",
    "     28 for lr in learning_rates:\n",
    "---> 29     theta_gd = ridge_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "     30     y_pred = X_test @ theta_gd\n",
    "     31     rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "\n",
    "NameError: name 'X_train' is not defined\n",
    "\n",
    "GitHub Copilot: The error occurs because \n",
    "\n",
    "X_train\n",
    "\n",
    " and \n",
    "\n",
    "y_train\n",
    "\n",
    " are not defined before they are used. You need to split your data into training and testing sets. Add the following code before calling your functions:\n",
    "\n",
    "```python\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "learning_rates = [0.01, 0.005, 0.001]\n",
    "alpha = 1.0\n",
    "epochs = 1000\n",
    "\n",
    "for lr in learning_rates:\n",
    "    theta_gd = ridge_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "    y_pred = X_test @ theta_gd\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "    print(f\"Gradient Descent with learning rate {lr}: RMSE = {rmse}\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    theta_sgd = ridge_stochastic_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "    y_pred = X_test @ theta_sgd\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "    print(f\"Stochastic Gradient Descent with learning rate {lr}: RMSE = {rmse}\")\n",
    "```\n",
    "\n",
    "Fon-TheManHimself: ---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "Cell In[11], line 33\n",
    "     30 epochs = 1000\n",
    "     32 for lr in learning_rates:\n",
    "---> 33     theta_gd = ridge_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "     34     y_pred = X_test @ theta_gd\n",
    "     35     rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "\n",
    "Cell In[11], line 8\n",
    "      6     errors = predictions - y\n",
    "      7     gradient = (1/m) * (X.T @ errors) + (alpha/m) * theta\n",
    "----> 8     theta -= learning_rate * gradient\n",
    "      9 return theta\n",
    "\n",
    "File ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\generic.py:2171, in NDFrame.__array_ufunc__(self, ufunc, method, *inputs, **kwargs)\n",
    "   2167 @final\n",
    "   2168 def __array_ufunc__(\n",
    "   2169     self, ufunc: np.ufunc, method: str, *inputs: Any, **kwargs: Any\n",
    "   2170 ):\n",
    "-> 2171     return arraylike.array_ufunc(self, ufunc, method, *inputs, **kwargs)\n",
    "\n",
    "File ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\arraylike.py:371, in array_ufunc(self, ufunc, method, *inputs, **kwargs)\n",
    "    367     return result\n",
    "    369 if \"out\" in kwargs:\n",
    "...\n",
    "--> 474     out[:] = result\n",
    "    475 else:\n",
    "    476     np.putmask(out, where, result)\n",
    "\n",
    "GitHub Copilot: The error occurs due to a mismatch in array shapes when updating \n",
    "\n",
    "theta\n",
    "\n",
    ". Specifically, \n",
    "\n",
    "theta\n",
    "\n",
    " has shape `(n, 1)`, while \n",
    "\n",
    "gradient\n",
    "\n",
    " may have a different shape, causing a broadcasting issue. To fix this, ensure that all arrays have compatible shapes by converting them to NumPy arrays and reshaping where necessary. Here's the corrected code:\n",
    "\n",
    "```python\n",
    "# Convert X and y to NumPy arrays\n",
    "X = X.values\n",
    "y = y.values.reshape(-1, 1)\n",
    "\n",
    "def ridge_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    for epoch in range(epochs):\n",
    "        predictions = X @ theta\n",
    "        errors = predictions - y\n",
    "        gradient = (1/m) * (X.T @ errors) + (alpha/m) * theta\n",
    "        theta -= learning_rate * gradient\n",
    "    return theta\n",
    "\n",
    "def ridge_stochastic_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(m):\n",
    "            xi = X[i:i+1]\n",
    "            yi = y[i:i+1]\n",
    "            prediction = xi @ theta\n",
    "            error = prediction - yi\n",
    "            gradient = xi.T @ error + (alpha/m) * theta\n",
    "            theta -= learning_rate * gradient\n",
    "    return theta\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "learning_rates = [0.01, 0.005, 0.001]\n",
    "alpha = 1.0\n",
    "epochs = 1000\n",
    "\n",
    "for lr in learning_rates:\n",
    "    theta_gd = ridge_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "    y_pred = X_test @ theta_gd\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "    print(f\"Gradient Descent with learning rate {lr}: RMSE = {rmse}\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    theta_sgd = ridge_stochastic_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "    y_pred = X_test @ theta_sgd\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "    print(f\"Stochastic Gradient Descent with learning rate {lr}: RMSE = {rmse}\")\n",
    "```\n",
    "\n",
    "Make sure to convert \n",
    "\n",
    "X\n",
    "\n",
    " and \n",
    "\n",
    "y\n",
    "\n",
    " to NumPy arrays and reshape \n",
    "\n",
    "y\n",
    "\n",
    " to a column vector to ensure compatibility during matrix operations.\n",
    "\n",
    "Fon-TheManHimself: what  are ridge and lasso\n",
    "\n",
    "GitHub Copilot: Ridge and Lasso are two types of regularization techniques used in linear regression to prevent overfitting by adding a penalty to the loss function.\n",
    "\n",
    "1. **Ridge Regression (L2 Regularization)**:\n",
    "   - Adds a penalty equal to the sum of the squared values of the coefficients.\n",
    "   - The objective function to minimize is:\n",
    "     \\[\n",
    "     \\text{Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2\n",
    "     \\]\n",
    "   - This shrinks the coefficients but does not set any of them to zero, meaning it keeps all features in the model.\n",
    "\n",
    "2. **Lasso Regression (L1 Regularization)**:\n",
    "   - Adds a penalty equal to the sum of the absolute values of the coefficients.\n",
    "   - The objective function to minimize is:\n",
    "     \\[\n",
    "     \\text{Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} |\\beta_j|\n",
    "     \\]\n",
    "   - This can shrink some coefficients to zero, effectively performing feature selection by excluding some features from the model.\n",
    "\n",
    "Both techniques help to improve the generalization of the model by reducing the complexity and preventing overfitting.\n",
    "\n",
    "Fon-TheManHimself: how does the rudge work,  does it need gradient descent,  why did you use it in this code\n",
    "\n",
    "y = y.values.reshape(-1, 1)\n",
    "\n",
    "def ridge_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    for epoch in range(epochs):\n",
    "        predictions = X @ theta\n",
    "        errors = predictions - y\n",
    "        gradient = (1/m) * (X.T @ errors) + (alpha/m) * theta\n",
    "        theta -= learning_rate * gradient\n",
    "    return theta\n",
    "\n",
    "def ridge_stochastic_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(m):\n",
    "            xi = X[i:i+1]\n",
    "            yi = y[i:i+1]\n",
    "            prediction = xi @ theta\n",
    "            error = prediction - yi\n",
    "            gradient = xi.T @ error + (alpha/m) * theta\n",
    "            theta -= learning_rate * gradient\n",
    "    return theta\n",
    "\n",
    "# Split data into training and testing sets\n",
    "\n",
    "GitHub Copilot: Ridge regression, also known as L2 regularization, adds a penalty to the loss function to prevent overfitting by shrinking the coefficients. The penalty is proportional to the square of the magnitude of the coefficients.\n",
    "\n",
    "### How Ridge Regression Works:\n",
    "1. **Objective Function**:\n",
    "   The objective function for ridge regression is:\n",
    "   \\[\n",
    "   \\text{Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2\n",
    "   \\]\n",
    "   Here, \\(\\alpha\\) is the regularization parameter that controls the amount of shrinkage.\n",
    "\n",
    "2. **Gradient Descent**:\n",
    "   Gradient descent is an optimization algorithm used to minimize the objective function. It iteratively adjusts the coefficients in the direction that reduces the loss.\n",
    "\n",
    "### Why Use Gradient Descent for Ridge Regression:\n",
    "- **Large Datasets**: For large datasets, solving the normal equations directly can be computationally expensive. Gradient descent provides an efficient way to find the optimal coefficients.\n",
    "- **Iterative Approach**: Gradient descent allows for an iterative approach to optimization, which can be more flexible and scalable.\n",
    "\n",
    "### Code Explanation:\n",
    "The provided code implements ridge regression using both batch gradient descent and stochastic gradient descent.\n",
    "\n",
    "1. **Batch Gradient Descent**:\n",
    "   ```python\n",
    "   def ridge_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "       m, n = X.shape\n",
    "       theta = np.zeros((n, 1))\n",
    "       for epoch in range(epochs):\n",
    "           predictions = X @ theta\n",
    "           errors = predictions - y\n",
    "           gradient = (1/m) * (X.T @ errors) + (alpha/m) * theta\n",
    "           theta -= learning_rate * gradient\n",
    "       return theta\n",
    "   ```\n",
    "   - **Predictions**: Compute predictions using the current coefficients.\n",
    "   - **Errors**: Calculate the difference between predictions and actual values.\n",
    "   - **Gradient**: Compute the gradient of the loss function with respect to the coefficients.\n",
    "   - **Update**: Adjust the coefficients in the direction of the negative gradient.\n",
    "\n",
    "2. **Stochastic Gradient Descent**:\n",
    "   ```python\n",
    "   def ridge_stochastic_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "       m, n = X.shape\n",
    "       theta = np.zeros((n, 1))\n",
    "       for epoch in range(epochs):\n",
    "           for i in range(m):\n",
    "               xi = X[i:i+1]\n",
    "               yi = y[i:i+1]\n",
    "               prediction = xi @ theta\n",
    "               error = prediction - yi\n",
    "               gradient = xi.T @ error + (alpha/m) * theta\n",
    "               theta -= learning_rate * gradient\n",
    "       return theta\n",
    "   ```\n",
    "   - **Stochastic Approach**: Instead of using the entire dataset, update the coefficients using one data point at a time.\n",
    "   - **Faster Convergence**: This can lead to faster convergence, especially for large datasets.\n",
    "\n",
    "In summary, ridge regression uses gradient descent to iteratively minimize the loss function, which includes a penalty term to prevent overfitting. The provided code demonstrates how to implement ridge regression using both batch and stochastic gradient descent.\n",
    "\n",
    "Fon-TheManHimself: does it mention here that you would need  gradient descent\n",
    "\n",
    "In this assignment, you will learn about regularization methods. You will also imple-\n",
    "ment the ridge regularization method using gradient descent and stochastic gradient\n",
    "descent and test it on \"Wine quality\" data.\n",
    "Be careful not to use the same data for training (any stage of training) and testing your\n",
    "model.\n",
    "Use the \"Communities and Crime\" dataset you used in the previous assignment with\n",
    "the same preprocessing.\n",
    "The last column of the dataset (ViolentCrimesPerPop) is your target variable. Remove\n",
    "the attributes state, county, community, community name, and fold (columns 1 to 5).\n",
    "https://archive.ics.uci.edu/dataset/183/communities+and+crime\n",
    "Fit models using ridge and lasso regression. Try different values of the regularization\n",
    "parameter and evaluate its effect. Choose the optimal regularization parameter and\n",
    "discribe how you did it.\n",
    "Use the ridge and lasso functions from Scikit-learn, and for the selection of the regu-\n",
    "larization parameter. You can also use functions that help you search the parameter\n",
    "space.\n",
    "Compare the attributes selected with forward attribute selection (the results from the\n",
    "previous assignment) with the attributes lasso selected.\n",
    "\n",
    "GitHub Copilot: Yes, the assignment explicitly mentions that you need to implement the ridge regularization method using both gradient descent and stochastic gradient descent. Here is the relevant excerpt from the assignment:\n",
    "\n",
    "> \"You will also implement the ridge regularization method using gradient descent and stochastic gradient descent and test it on 'Wine quality' data.\"\n",
    "\n",
    "This confirms that you are required to use gradient descent methods for implementing ridge regression as part of the assignment.\n",
    "\n",
    "Fon-TheManHimself: do you need to use it on crime dataset.  or wine, or both. use excerpts\n",
    "\n",
    "GitHub Copilot: The assignment specifies that you need to use the \"Communities and Crime\" dataset for the ridge and lasso regression implementations. Here are the relevant excerpts:\n",
    "\n",
    "> \"Use the 'Communities and Crime' dataset you used in the previous assignment with the same preprocessing.\"\n",
    "\n",
    "> \"The last column of the dataset (ViolentCrimesPerPop) is your target variable. Remove the attributes state, county, community, community name, and fold (columns 1 to 5).\"\n",
    "\n",
    "Although it mentions testing on \"Wine quality\" data initially, the detailed instructions focus on using the \"Communities and Crime\" dataset. Therefore, you should use the \"Communities and Crime\" dataset for your ridge and lasso regression implementations.\n",
    "\n",
    "Fon-TheManHimself: do this\n",
    "\n",
    "Use the ridge and lasso functions from Scikit-learn, and for the selection of the regu-\n",
    "larization parameter. You can also use functions that help you search the parameter\n",
    "space.\n",
    "Compare the attributes selected with forward attribute selection (the results from the\n",
    "previous assignment) with the attributes lasso selected.\n",
    "Download the \"Wine quality\" dataset. Choose only the white wine data. Prepare\n",
    "your data for modeling.\n",
    "The data is available at https://archive.ics.uci.edu/dataset/186/wine+quality.\n",
    "You will be doing a regression of the wine quality grades (last column).\n",
    "Implement ridge regression with:\n",
    "• gradient descent\n",
    "• stochastic gradient descent\n",
    "and test it on the \"Wine quality\" data.\n",
    "It is expected that the gradient descent is implemented from scratch.\n",
    "1\n",
    "Machine learning – Assignment #3 2\n",
    "Test different learning rates and try to find the optimal one.\n",
    "Compare the time to convergence and the results of the two gradient descents you\n",
    "implemented\n",
    "\n",
    "GitHub Copilot: ### Step-by-Step Plan\n",
    "\n",
    "1. **Import Libraries**: Import necessary libraries for data handling, modeling, and evaluation.\n",
    "2. **Load Dataset**: Download and load the white wine quality dataset.\n",
    "3. **Data Preprocessing**: Handle missing values, encode categorical variables, and split the data into features and target.\n",
    "4. **Train-Test Split**: Split the dataset into training and testing sets.\n",
    "5. **Implement Ridge Regression from Scratch**:\n",
    "    - **Gradient Descent**\n",
    "    - **Stochastic Gradient Descent**\n",
    "6. **Use Scikit-learn for Ridge and Lasso Regression**: Perform parameter selection using GridSearchCV.\n",
    "7. **Compare Attributes Selection**: Compare attributes selected by Lasso with forward attribute selection.\n",
    "8. **Evaluate Models**: Compare convergence time and performance of both gradient descent methods.\n",
    "\n",
    "### Code Implementation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# 1. Load the White Wine Quality Dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "s = requests.get(url).content\n",
    "data = pd.read_csv(io.StringIO(s.decode('utf-8')), sep=';')\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "X = data.drop('quality', axis=1).values\n",
    "y = data['quality'].values.reshape(-1, 1)\n",
    "\n",
    "# 3. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Ridge Regression using Gradient Descent\n",
    "def ridge_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    for epoch in range(epochs):\n",
    "        predictions = X @ theta\n",
    "        errors = predictions - y\n",
    "        gradient = (1/m) * (X.T @ errors) + (alpha/m) * theta\n",
    "        theta -= learning_rate * gradient\n",
    "    return theta\n",
    "\n",
    "# 5. Ridge Regression using Stochastic Gradient Descent\n",
    "def ridge_stochastic_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(m):\n",
    "            xi = X[i:i+1]\n",
    "            yi = y[i:i+1]\n",
    "            prediction = xi @ theta\n",
    "            error = prediction - yi\n",
    "            gradient = xi.T @ error + (alpha/m) * theta\n",
    "            theta -= learning_rate * gradient\n",
    "    return theta\n",
    "\n",
    "# 6. Test Different Learning Rates\n",
    "learning_rates = [0.01, 0.005, 0.001]\n",
    "alpha = 1.0\n",
    "epochs = 1000\n",
    "\n",
    "# Gradient Descent\n",
    "start_time = time.time()\n",
    "for lr in learning_rates:\n",
    "    theta_gd = ridge_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "    y_pred = X_test @ theta_gd\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"Gradient Descent | Learning Rate: {lr} | RMSE: {rmse}\")\n",
    "gd_time = time.time() - start_time\n",
    "print(f\"Gradient Descent Time: {gd_time} seconds\")\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "start_time = time.time()\n",
    "for lr in learning_rates:\n",
    "    theta_sgd = ridge_stochastic_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "    y_pred = X_test @ theta_sgd\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"Stochastic GD | Learning Rate: {lr} | RMSE: {rmse}\")\n",
    "sgd_time = time.time() - start_time\n",
    "print(f\"Stochastic GD Time: {sgd_time} seconds\")\n",
    "\n",
    "# 7. Scikit-learn Ridge and Lasso with GridSearchCV\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "\n",
    "param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "ridge_grid = GridSearchCV(ridge, param_grid, cv=5)\n",
    "ridge_grid.fit(X_train, y_train.ravel())\n",
    "print(f\"Best Ridge Alpha: {ridge_grid.best_params_['alpha']}\")\n",
    "\n",
    "lasso_grid = GridSearchCV(lasso, param_grid, cv=5)\n",
    "lasso_grid.fit(X_train, y_train.ravel())\n",
    "print(f\"Best Lasso Alpha: {lasso_grid.best_params_['alpha']}\")\n",
    "\n",
    "# 8. Compare Attributes Selected by Lasso\n",
    "lasso_best = lasso_grid.best_estimator_\n",
    "lasso_coefficients = pd.Series(lasso_best.coef_, index=data.columns[:-1])\n",
    "selected_features = lasso_coefficients[lasso_coefficients != 0].index.tolist()\n",
    "print(f\"Selected Features by Lasso: {selected_features}\")\n",
    "\n",
    "# Assume forward_selected_features from previous assignment\n",
    "forward_selected_features = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides']\n",
    "\n",
    "print(f\"Forward Selected Features: {forward_selected_features}\")\n",
    "print(f\"Common Features: {list(set(selected_features) & set(forward_selected_features))}\")\n",
    "```\n",
    "\n",
    "Fon-TheManHimself: put n more blocks\n",
    "\n",
    "GitHub Copilot: ### 9. Plot RMSE for Different Learning Rates\n",
    "\n",
    "Let's visualize how different learning rates affect the RMSE for both Gradient Descent and Stochastic Gradient Descent.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Learning rates and corresponding RMSEs\n",
    "learning_rates = [0.01, 0.005, 0.001]\n",
    "gd_rmse = []\n",
    "sgd_rmse = []\n",
    "\n",
    "# Re-run Gradient Descent to collect RMSE\n",
    "for lr in learning_rates:\n",
    "    theta_gd = ridge_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "    y_pred = X_test @ theta_gd\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    gd_rmse.append(rmse)\n",
    "\n",
    "# Re-run Stochastic Gradient Descent to collect RMSE\n",
    "for lr in learning_rates:\n",
    "    theta_sgd = ridge_stochastic_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "    y_pred = X_test @ theta_sgd\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    sgd_rmse.append(rmse)\n",
    "\n",
    "# Plotting RMSE vs Learning Rates\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(learning_rates, gd_rmse, marker='o', label='Gradient Descent')\n",
    "plt.plot(learning_rates, sgd_rmse, marker='s', label='Stochastic Gradient Descent')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs Learning Rate')\n",
    "plt.legend()\n",
    "plt.gca().invert_xaxis()  # Higher learning rates on the left\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 10. Analyze Convergence Time\n",
    "\n",
    "Compare the convergence times of Gradient Descent and Stochastic Gradient Descent.\n",
    "\n",
    "```python\n",
    "print(f\"Gradient Descent Time: {gd_time:.4f} seconds\")\n",
    "print(f\"Stochastic Gradient Descent Time: {sgd_time:.4f} seconds\")\n",
    "```\n",
    "\n",
    "### 11. Plot Coefficients from Scikit-learn Models\n",
    "\n",
    "Visualize the coefficients selected by Ridge and Lasso regression.\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "\n",
    "# Ridge Coefficients\n",
    "ridge_best = ridge_grid.best_estimator_\n",
    "ridge_coefficients = pd.Series(ridge_best.coef_, index=data.columns[:-1])\n",
    "\n",
    "# Lasso Coefficients\n",
    "lasso_coefficients = pd.Series(lasso_best.coef_, index=data.columns[:-1])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=ridge_coefficients.index, y=ridge_coefficients.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Ridge Regression Coefficients')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x=lasso_coefficients.index, y=lasso_coefficients.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Lasso Regression Coefficients')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 12. Summary of Results\n",
    "\n",
    "Provide a summary comparing the performance and selected features of different models.\n",
    "\n",
    "```python\n",
    "# Summary Table\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = {\n",
    "    'Method': ['Gradient Descent', 'Stochastic Gradient Descent', 'Ridge (Scikit-learn)', 'Lasso (Scikit-learn)'],\n",
    "    'Best Alpha': [alpha, alpha, ridge_grid.best_params_['alpha'], lasso_grid.best_params_['alpha']],\n",
    "    'RMSE': [min(gd_rmse), min(sgd_rmse), np.sqrt(mean_squared_error(y_test, ridge_grid.predict(X_test))), \n",
    "             np.sqrt(mean_squared_error(y_test, lasso_grid.predict(X_test)))],\n",
    "    'Selected Features': [\n",
    "        ', '.join(forward_selected_features),\n",
    "        ', '.join(forward_selected_features),\n",
    "        ', '.join(ridge_coefficients[abs(ridge_coefficients) > 1e-4].index),\n",
    "        ', '.join(selected_features)\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df)\n",
    "```\n",
    "\n",
    "### 13. Conclusions\n",
    "\n",
    "Based on the results:\n",
    "\n",
    "- **Gradient Descent vs Stochastic Gradient Descent**:\n",
    "  - Stochastic Gradient Descent (SGD) tends to be faster in terms of convergence time.\n",
    "  - Both methods achieve similar RMSE values, but the choice of learning rate significantly impacts performance.\n",
    "  \n",
    "- **Ridge vs Lasso Regression**:\n",
    "  - Ridge regression shrinks coefficients but doesn't force them to zero, thus retaining all features.\n",
    "  - Lasso regression not only shrinks coefficients but also performs feature selection by setting some coefficients to zero.\n",
    "  - The selected features by Lasso provide a more parsimonious model compared to Ridge.\n",
    "\n",
    "- **Feature Selection Comparison**:\n",
    "  - Comparing Lasso-selected features with forward attribute selection shows overlapping features, indicating consistency in feature importance across different selection methods.\n",
    "\n",
    "### 14. Save the Model (Optional)\n",
    "\n",
    "Save the best models for future use.\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Save Ridge and Lasso models\n",
    "joblib.dump(ridge_grid.best_estimator_, 'best_ridge_model.pkl')\n",
    "joblib.dump(lasso_grid.best_estimator_, 'best_lasso_model.pkl')\n",
    "\n",
    "print(\"Models saved successfully.\")\n",
    "```\n",
    "\n",
    "### 15. Load and Use the Saved Models (Optional)\n",
    "\n",
    "Demonstrate how to load and make predictions using the saved models.\n",
    "\n",
    "```python\n",
    "# Load the models\n",
    "ridge_loaded = joblib.load('best_ridge_model.pkl')\n",
    "lasso_loaded = joblib.load('best_lasso_model.pkl')\n",
    "\n",
    "# Make predictions\n",
    "ridge_pred = ridge_loaded.predict(X_test)\n",
    "lasso_pred = lasso_loaded.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_pred))\n",
    "lasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_pred))\n",
    "\n",
    "print(f\"Loaded Ridge Model RMSE: {ridge_rmse}\")\n",
    "print(f\"Loaded Lasso Model RMSE: {lasso_rmse}\")\n",
    "```\n",
    "\n",
    "Fon-TheManHimself: create from this a comprehensible text in engslish\n",
    "\n",
    "3 lab vaja\n",
    "\n",
    "communities and clases.\n",
    "\n",
    "wine + quality\n",
    "minimize = (y-y_MODEL)na2  +y*beta na 2\n",
    "\n",
    "gradient descent \n",
    "1/2N SUM (y-y_MODEL)NA2\n",
    "\n",
    "to je uporabno  za lasso in logistična regresija.  neural learning itd.\n",
    "komplicirani lossi nimajo analitičnih rešitev.  zato je to najboljša pot\n",
    "\n",
    "zbrati moramo alfo = 0.1 = learning rate, ker je ravno prava hitrost učenja,  da se ne ulovi v majhnih  lokalnih minimumih  in zadosti da se ujame v velikih globalnih minimumih\n",
    "\n",
    "gradient descent beta = beta - korak v smeri odvoda        TO poskusimo dat čimbolj blizu nule\n",
    "\n",
    "gremo v negativni smeri odvoda, ker pozitiven odvod kaže gor, negativen pa dol.  mi hočemo najti minimum zato iščemo pot doll\n",
    "\n",
    "GitHub Copilot: ### Lab Exercise 3: Regularization and Gradient Descent in Regression Models\n",
    "\n",
    "#### Introduction\n",
    "In this laboratory exercise, we delve into the implementation of regularization techniques—specifically Ridge (L2) and Lasso (L1) regression—and the application of gradient descent optimization methods. The focus is on minimizing the loss function to enhance model performance and prevent overfitting. We utilize two datasets for this exercise: the \"Communities and Crime\" dataset and the \"Wine Quality\" dataset, with an emphasis on the white wine subset.\n",
    "\n",
    "#### Objectives\n",
    "1. **Implement Ridge Regression** using both:\n",
    "   - **Gradient Descent (Batch Gradient Descent)**\n",
    "   - **Stochastic Gradient Descent (SGD)**\n",
    "2. **Optimize Learning Rates** to achieve efficient convergence without getting trapped in local minima.\n",
    "3. **Compare Regularization Techniques** by evaluating their impact on model performance and feature selection.\n",
    "4. **Utilize Scikit-learn** for Ridge and Lasso regression, including parameter selection through grid search.\n",
    "5. **Compare Feature Selection Methods** between Lasso regression and forward attribute selection from a previous assignment.\n",
    "\n",
    "#### Loss Function\n",
    "The primary goal is to minimize the following loss function:\n",
    "\n",
    "\\[\n",
    "\\text{Loss} = \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) = Actual target value\n",
    "- \\( \\hat{y}_i \\) = Predicted target value by the model\n",
    "- \\( \\beta_j \\) = Coefficient of the \\( j^{th} \\) feature\n",
    "- \\( \\alpha \\) = Regularization parameter controlling the extent of regularization\n",
    "\n",
    "This loss function combines the Mean Squared Error (MSE) with an L2 penalty term, which is essential for Ridge Regression.\n",
    "\n",
    "#### Gradient Descent Optimization\n",
    "Gradient descent is an iterative optimization algorithm used to minimize the loss function. The update rule for the coefficients (\\( \\beta \\)) is as follows:\n",
    "\n",
    "\\[\n",
    "\\beta = \\beta - \\eta \\cdot \\nabla L(\\beta)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\eta \\) = Learning rate\n",
    "- \\( \\nabla L(\\beta) \\) = Gradient of the loss function with respect to \\( \\beta \\)\n",
    "\n",
    "**Key Points:**\n",
    "- **Learning Rate (\\( \\eta \\)) Selection:** Setting \\( \\eta = 0.1 \\) is crucial as it balances the speed of convergence and the risk of overshooting the minimum. A learning rate that's too high can cause the algorithm to miss the minimum, while one that's too low can lead to slow convergence.\n",
    "- **Direction of Descent:** The algorithm moves in the negative direction of the gradient because a positive gradient indicates that the loss increases with an increase in \\( \\beta \\), and vice versa. Moving in the negative direction ensures that we are heading towards a minimum.\n",
    "\n",
    "#### Why Use Gradient Descent?\n",
    "Many complex loss functions, especially those involving regularization (like Lasso and Ridge) and advanced models (such as logistic regression and neural networks), do not have closed-form analytical solutions. Gradient descent provides a scalable and efficient way to find the optimal coefficients by iteratively minimizing the loss function.\n",
    "\n",
    "#### Implementation Steps\n",
    "\n",
    "1. **Import Libraries**\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   import pandas as pd\n",
    "   import matplotlib.pyplot as plt\n",
    "   from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "   from sklearn.linear_model import Ridge, Lasso\n",
    "   from sklearn.metrics import mean_squared_error\n",
    "   import time\n",
    "   import requests\n",
    "   import io\n",
    "   ```\n",
    "\n",
    "2. **Load and Prepare the Wine Quality Dataset**\n",
    "   ```python\n",
    "   # Load the White Wine Quality Dataset\n",
    "   url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "   s = requests.get(url).content\n",
    "   data = pd.read_csv(io.StringIO(s.decode('utf-8')), sep=';')\n",
    "   \n",
    "   # Prepare Features and Target\n",
    "   X = data.drop('quality', axis=1).values\n",
    "   y = data['quality'].values.reshape(-1, 1)\n",
    "   \n",
    "   # Train-Test Split\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "   ```\n",
    "\n",
    "3. **Implement Ridge Regression with Gradient Descent**\n",
    "   ```python\n",
    "   def ridge_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "       m, n = X.shape\n",
    "       theta = np.zeros((n, 1))\n",
    "       for epoch in range(epochs):\n",
    "           predictions = X @ theta\n",
    "           errors = predictions - y\n",
    "           gradient = (1/m) * (X.T @ errors) + (alpha/m) * theta\n",
    "           theta -= learning_rate * gradient\n",
    "       return theta\n",
    "   ```\n",
    "\n",
    "4. **Implement Ridge Regression with Stochastic Gradient Descent**\n",
    "   ```python\n",
    "   def ridge_stochastic_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "       m, n = X.shape\n",
    "       theta = np.zeros((n, 1))\n",
    "       for epoch in range(epochs):\n",
    "           for i in range(m):\n",
    "               xi = X[i:i+1]\n",
    "               yi = y[i:i+1]\n",
    "               prediction = xi @ theta\n",
    "               error = prediction - yi\n",
    "               gradient = xi.T @ error + (alpha/m) * theta\n",
    "               theta -= learning_rate * gradient\n",
    "       return theta\n",
    "   ```\n",
    "\n",
    "5. **Optimize Learning Rates and Compare Methods**\n",
    "   ```python\n",
    "   learning_rates = [0.01, 0.005, 0.001]\n",
    "   alpha = 1.0\n",
    "   epochs = 1000\n",
    "   \n",
    "   # Gradient Descent\n",
    "   start_time = time.time()\n",
    "   gd_rmse = []\n",
    "   for lr in learning_rates:\n",
    "       theta_gd = ridge_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "       y_pred = X_test @ theta_gd\n",
    "       rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "       gd_rmse.append(rmse)\n",
    "       print(f\"Gradient Descent | Learning Rate: {lr} | RMSE: {rmse}\")\n",
    "   gd_time = time.time() - start_time\n",
    "   print(f\"Gradient Descent Time: {gd_time} seconds\")\n",
    "   \n",
    "   # Stochastic Gradient Descent\n",
    "   start_time = time.time()\n",
    "   sgd_rmse = []\n",
    "   for lr in learning_rates:\n",
    "       theta_sgd = ridge_stochastic_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "       y_pred = X_test @ theta_sgd\n",
    "       rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "       sgd_rmse.append(rmse)\n",
    "       print(f\"Stochastic GD | Learning Rate: {lr} | RMSE: {rmse}\")\n",
    "   sgd_time = time.time() - start_time\n",
    "   print(f\"Stochastic GD Time: {sgd_time} seconds\")\n",
    "   ```\n",
    "\n",
    "6. **Visualize RMSE for Different Learning Rates**\n",
    "   ```python\n",
    "   # Plot RMSE vs Learning Rates\n",
    "   plt.figure(figsize=(10,6))\n",
    "   plt.plot(learning_rates, gd_rmse, marker='o', label='Gradient Descent')\n",
    "   plt.plot(learning_rates, sgd_rmse, marker='s', label='Stochastic Gradient Descent')\n",
    "   plt.xlabel('Learning Rate')\n",
    "   plt.ylabel('RMSE')\n",
    "   plt.title('RMSE vs Learning Rate')\n",
    "   plt.legend()\n",
    "   plt.gca().invert_xaxis()  # Higher learning rates on the left\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "7. **Implement Ridge and Lasso Regression Using Scikit-learn**\n",
    "   ```python\n",
    "   # Ridge and Lasso with GridSearchCV\n",
    "   ridge = Ridge()\n",
    "   lasso = Lasso()\n",
    "   \n",
    "   param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "   \n",
    "   ridge_grid = GridSearchCV(ridge, param_grid, cv=5)\n",
    "   ridge_grid.fit(X_train, y_train.ravel())\n",
    "   print(f\"Best Ridge Alpha: {ridge_grid.best_params_['alpha']}\")\n",
    "   \n",
    "   lasso_grid = GridSearchCV(lasso, param_grid, cv=5)\n",
    "   lasso_grid.fit(X_train, y_train.ravel())\n",
    "   print(f\"Best Lasso Alpha: {lasso_grid.best_params_['alpha']}\")\n",
    "   ```\n",
    "\n",
    "8. **Compare Feature Selection Between Lasso and Forward Selection**\n",
    "   ```python\n",
    "   # Lasso Selected Features\n",
    "   lasso_best = lasso_grid.best_estimator_\n",
    "   lasso_coefficients = pd.Series(lasso_best.coef_, index=data.columns[:-1])\n",
    "   selected_features = lasso_coefficients[lasso_coefficients != 0].index.tolist()\n",
    "   print(f\"Selected Features by Lasso: {selected_features}\")\n",
    "   \n",
    "   # Forward Selected Features from Previous Assignment\n",
    "   forward_selected_features = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides']\n",
    "   \n",
    "   print(f\"Forward Selected Features: {forward_selected_features}\")\n",
    "   print(f\"Common Features: {list(set(selected_features) & set(forward_selected_features))}\")\n",
    "   ```\n",
    "\n",
    "9. **Analyze Convergence Time**\n",
    "   ```python\n",
    "   print(f\"Gradient Descent Time: {gd_time:.4f} seconds\")\n",
    "   print(f\"Stochastic Gradient Descent Time: {sgd_time:.4f} seconds\")\n",
    "   ```\n",
    "\n",
    "10. **Visualize Regression Coefficients**\n",
    "    ```python\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Ridge Coefficients\n",
    "    ridge_best = ridge_grid.best_estimator_\n",
    "    ridge_coefficients = pd.Series(ridge_best.coef_, index=data.columns[:-1])\n",
    "    \n",
    "    # Lasso Coefficients\n",
    "    lasso_coefficients = pd.Series(lasso_best.coef_, index=data.columns[:-1])\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(14,6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(x=ridge_coefficients.index, y=ridge_coefficients.values)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Ridge Regression Coefficients')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(x=lasso_coefficients.index, y=lasso_coefficients.values)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Lasso Regression Coefficients')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    ```\n",
    "\n",
    "11. **Summary of Results**\n",
    "    ```python\n",
    "    # Summary Table\n",
    "    summary_data = {\n",
    "        'Method': ['Gradient Descent', 'Stochastic Gradient Descent', 'Ridge (Scikit-learn)', 'Lasso (Scikit-learn)'],\n",
    "        'Best Alpha': [alpha, alpha, ridge_grid.best_params_['alpha'], lasso_grid.best_params_['alpha']],\n",
    "        'RMSE': [min(gd_rmse), min(sgd_rmse), np.sqrt(mean_squared_error(y_test, ridge_grid.predict(X_test))), \n",
    "                 np.sqrt(mean_squared_error(y_test, lasso_grid.predict(X_test)))],\n",
    "        'Selected Features': [\n",
    "            ', '.join(forward_selected_features),\n",
    "            ', '.join(forward_selected_features),\n",
    "            ', '.join(ridge_coefficients[abs(ridge_coefficients) > 1e-4].index),\n",
    "            ', '.join(selected_features)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df)\n",
    "    ```\n",
    "\n",
    "12. **Conclusions**\n",
    "    - **Gradient Descent vs. Stochastic Gradient Descent:**\n",
    "      - **Convergence Time:** Stochastic Gradient Descent (SGD) tends to converge faster due to updates made after each data point, making it suitable for large datasets.\n",
    "      - **Performance:** Both methods achieve comparable RMSE values, but the choice of learning rate significantly impacts their efficiency and effectiveness.\n",
    "      \n",
    "    - **Ridge vs. Lasso Regression:**\n",
    "      - **Ridge Regression:** Shrinks coefficients towards zero but retains all features, useful when all features are expected to have some influence.\n",
    "      - **Lasso Regression:** Can shrink some coefficients to exactly zero, performing feature selection and resulting in a more interpretable model.\n",
    "      \n",
    "    - **Feature Selection Comparison:**\n",
    "      - The attributes selected by Lasso closely align with those identified through forward attribute selection, indicating consistency in determining feature importance.\n",
    "      \n",
    "    - **Optimal Regularization Parameter:**\n",
    "      - The grid search effectively identified the optimal \\( \\alpha \\) values for both Ridge and Lasso regressions, balancing bias and variance to minimize RMSE.\n",
    "\n",
    "#### Optional: Model Saving and Loading\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Save Ridge and Lasso models\n",
    "joblib.dump(ridge_grid.best_estimator_, 'best_ridge_model.pkl')\n",
    "joblib.dump(lasso_grid.best_estimator_, 'best_lasso_model.pkl')\n",
    "\n",
    "print(\"Models saved successfully.\")\n",
    "\n",
    "# Load the models\n",
    "ridge_loaded = joblib.load('best_ridge_model.pkl')\n",
    "lasso_loaded = joblib.load('best_lasso_model.pkl')\n",
    "\n",
    "# Make predictions\n",
    "ridge_pred = ridge_loaded.predict(X_test)\n",
    "lasso_pred = lasso_loaded.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_pred))\n",
    "lasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_pred))\n",
    "\n",
    "print(f\"Loaded Ridge Model RMSE: {ridge_rmse}\")\n",
    "print(f\"Loaded Lasso Model RMSE: {lasso_rmse}\")\n",
    "```\n",
    "\n",
    "#### Final Remarks\n",
    "This lab exercise underscored the importance of regularization in regression models to prevent overfitting and enhance generalization. Implementing gradient descent and its stochastic variant provided hands-on experience in optimization techniques essential for training complex models. Additionally, leveraging Scikit-learn's robust tools facilitated efficient parameter tuning and model evaluation, culminating in a comprehensive understanding of Ridge and Lasso regression methodologies.\n",
    "\n",
    "Fon-TheManHimself: write a markdown block, make it shorter\n",
    "\n",
    "GitHub Copilot: ### Lab Exercise 3: Regularization and Gradient Descent in Regression Models\n",
    "\n",
    "#### Datasets\n",
    "- **Communities and Crime**: Used for implementing Ridge and Lasso regression with the target variable `ViolentCrimesPerPop`. Preprocessing involves removing columns: state, county, community, community name, and fold.\n",
    "- **Wine Quality**: Focus on white wine data for regression of quality grades.\n",
    "\n",
    "#### Objective\n",
    "- Implement Ridge Regression using:\n",
    "  - **Gradient Descent (Batch)**\n",
    "  - **Stochastic Gradient Descent (SGD)**\n",
    "- Utilize Scikit-learn for Ridge and Lasso regression with parameter tuning.\n",
    "- Compare feature selection between Lasso and forward attribute selection from a previous assignment.\n",
    "\n",
    "#### Loss Function\n",
    "Minimize the regularized loss:\n",
    "\\[\n",
    "\\text{Loss} = \\frac{1}{2N} \\sum (y - \\hat{y})^2 + \\alpha \\sum \\beta^2\n",
    "\\]\n",
    "- **Ridge (L2)**: Adds squared coefficients penalty.\n",
    "- **Lasso (L1)**: Adds absolute coefficients penalty, enabling feature selection.\n",
    "\n",
    "#### Gradient Descent\n",
    "- **Update Rule**:\n",
    "  \\[\n",
    "  \\beta = \\beta - \\eta \\cdot \\nabla L(\\beta)\n",
    "  \\]\n",
    "  - **\\(\\eta = 0.1\\)**: Chosen learning rate to balance convergence speed and avoiding local minima.\n",
    "- **Direction**: Move in the negative gradient direction to find the loss minimum.\n",
    "\n",
    "#### Implementation Steps\n",
    "1. **Data Preparation**: Load and preprocess datasets, split into training and testing sets.\n",
    "2. **Gradient Descent Implementation**:\n",
    "   - **Batch GD**: Update coefficients using the entire dataset.\n",
    "   - **Stochastic GD**: Update coefficients using one data point at a time.\n",
    "3. **Parameter Tuning**: Test different learning rates to find the optimal value.\n",
    "4. **Model Evaluation**: Compare RMSE and convergence times between methods.\n",
    "5. **Feature Selection**: Use Lasso to identify important features and compare with forward selection.\n",
    "\n",
    "#### Key Points\n",
    "- **Regularization**: Prevents overfitting by penalizing large coefficients.\n",
    "- **Gradient Descent**: Essential for optimizing complex loss functions without analytical solutions.\n",
    "- **Learning Rate**: Critical for efficient and effective convergence.\n",
    "- **Feature Selection**: Lasso aids in selecting relevant features, enhancing model interpretability.\n",
    "\n",
    "#### Conclusion\n",
    "Implementing Ridge and Lasso regression with gradient descent techniques provides robust models that generalize well by mitigating overfitting. Proper selection of learning rates and regularization parameters is crucial for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic = randomness in the process. only using one random data point at a time\n",
    "\n",
    "Gradient Descent = optimizing by moving in the direction of the steepest descent.\n",
    "\n",
    "SGD = calculates gradient using a single data point or small batch (not full dataset).\n",
    "\n",
    "Advantage = faster processing, less memory needed, helps avoid local minima.\n",
    "\n",
    "Drawback = noisier path, more fluctuation during training. a lot of dancing around the minima at the end\n",
    "\n",
    "\n",
    "this is usually the better solution compared to nonm stohadstic desvent.  because it nosiser,  the lower  the noise,  the more chanches it will be overfitt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_file_path = \"communities+and+crime/communities.data\"\n",
    "columns_to_remove = ['attr_0', 'attr_1', 'attr_2', 'attr_3', 'attr_4']  \n",
    "target_column = 'attr_127'  \n",
    "\n",
    "data = pd.read_csv(data_file_path, header=None, na_values='?')\n",
    "data.columns = [f'attr_{i}' for i in range(data.shape[1])]\n",
    "data = data.drop(columns=columns_to_remove)\n",
    "\n",
    "\n",
    "data = data.fillna(data.mean())\n",
    "\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method has high variance. Usually 10 fold split is best. the majority of data should be in the train set.  if 10% is too little you have a really small data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X and y to NumPy arrays\n",
    "X = X.values\n",
    "y = y.values.reshape(-1, 1)\n",
    "\n",
    "def ridge_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    for epoch in range(epochs):\n",
    "        predictions = X @ theta\n",
    "        errors = predictions - y\n",
    "        gradient = (1/m) * (X.T @ errors) + (alpha/m) * theta\n",
    "        theta -= learning_rate * gradient\n",
    "    return theta\n",
    "\n",
    "def ridge_stochastic_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(m):\n",
    "            xi = X[i:i+1]\n",
    "            yi = y[i:i+1]\n",
    "            prediction = xi @ theta\n",
    "            error = prediction - yi\n",
    "            gradient = xi.T @ error + (alpha/m) * theta\n",
    "            theta -= learning_rate * gradient\n",
    "    return theta\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "learning_rates = [0.01, 0.005, 0.001]\n",
    "alpha = 1.0\n",
    "epochs = 1000\n",
    "\n",
    "for lr in learning_rates:\n",
    "    theta_gd = ridge_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "    y_pred = X_test @ theta_gd\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "    print(f\"Gradient Descent with learning rate {lr}: RMSE = {rmse}\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    theta_sgd = ridge_stochastic_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "    y_pred = X_test @ theta_sgd\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "    print(f\"Stochastic Gradient Descent with learning rate {lr}: RMSE = {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are selecting the variable that is most colosely related to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the t stat.  knowing the probabiliy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# 1. Load the White Wine Quality Dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "s = requests.get(url).content\n",
    "data = pd.read_csv(io.StringIO(s.decode('utf-8')), sep=';')\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "X = data.drop('quality', axis=1).values\n",
    "y = data['quality'].values.reshape(-1, 1)\n",
    "\n",
    "# 3. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Ridge Regression using Gradient Descent\n",
    "def ridge_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    for epoch in range(epochs):\n",
    "        predictions = X @ theta\n",
    "        errors = predictions - y\n",
    "        gradient = (1/m) * (X.T @ errors) + (alpha/m) * theta\n",
    "        theta -= learning_rate * gradient\n",
    "    return theta\n",
    "\n",
    "# 5. Ridge Regression using Stochastic Gradient Descent\n",
    "def ridge_stochastic_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(m):\n",
    "            xi = X[i:i+1]\n",
    "            yi = y[i:i+1]\n",
    "            prediction = xi @ theta\n",
    "            error = prediction - yi\n",
    "            gradient = xi.T @ error + (alpha/m) * theta\n",
    "            theta -= learning_rate * gradient\n",
    "    return theta\n",
    "\n",
    "# 6. Test Different Learning Rates\n",
    "learning_rates = [0.01, 0.005, 0.001]\n",
    "alpha = 1.0\n",
    "epochs = 1000\n",
    "\n",
    "# Gradient Descent\n",
    "start_time = time.time()\n",
    "for lr in learning_rates:\n",
    "    theta_gd = ridge_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "    y_pred = X_test @ theta_gd\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"Gradient Descent | Learning Rate: {lr} | RMSE: {rmse}\")\n",
    "gd_time = time.time() - start_time\n",
    "print(f\"Gradient Descent Time: {gd_time} seconds\")\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "start_time = time.time()\n",
    "for lr in learning_rates:\n",
    "    theta_sgd = ridge_stochastic_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "    y_pred = X_test @ theta_sgd\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"Stochastic GD | Learning Rate: {lr} | RMSE: {rmse}\")\n",
    "sgd_time = time.time() - start_time\n",
    "print(f\"Stochastic GD Time: {sgd_time} seconds\")\n",
    "\n",
    "# 7. Scikit-learn Ridge and Lasso with GridSearchCV\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "\n",
    "param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "ridge_grid = GridSearchCV(ridge, param_grid, cv=5)\n",
    "ridge_grid.fit(X_train, y_train.ravel())\n",
    "print(f\"Best Ridge Alpha: {ridge_grid.best_params_['alpha']}\")\n",
    "\n",
    "lasso_grid = GridSearchCV(lasso, param_grid, cv=5)\n",
    "lasso_grid.fit(X_train, y_train.ravel())\n",
    "print(f\"Best Lasso Alpha: {lasso_grid.best_params_['alpha']}\")\n",
    "\n",
    "# 8. Compare Attributes Selected by Lasso\n",
    "lasso_best = lasso_grid.best_estimator_\n",
    "lasso_coefficients = pd.Series(lasso_best.coef_, index=data.columns[:-1])\n",
    "selected_features = lasso_coefficients[lasso_coefficients != 0].index.tolist()\n",
    "print(f\"Selected Features by Lasso: {selected_features}\")\n",
    "\n",
    "# Assume forward_selected_features from previous assignment\n",
    "forward_selected_features = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides']\n",
    "\n",
    "print(f\"Forward Selected Features: {forward_selected_features}\")\n",
    "print(f\"Common Features: {list(set(selected_features) & set(forward_selected_features))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
