{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 lab vaja\n",
    "\n",
    "communities and clases.\n",
    "\n",
    "wine + quality\n",
    "minimize = (y-y_MODEL)na2  +y*beta na 2\n",
    "\n",
    "gradient descent \n",
    "1/2N SUM (y-y_MODEL)NA2\n",
    "\n",
    "to je uporabno  za lasso in logistična regresija.  neural learning itd.\n",
    "komplicirani lossi nimajo analitičnih rešitev.  zato je to najboljša pot\n",
    "\n",
    "zbrati moramo alfo = 0.1 = learning rate, ker je ravno prava hitrost učenja,  da se ne ulovi v majhnih  lokalnih minimumih  in zadosti da se ujame v velikih globalnih minimumih\n",
    "\n",
    "gradient descent beta = beta - korak v smeri odvoda        TO poskusimo dat čimbolj blizu nule\n",
    "\n",
    "gremo v negativni smeri odvoda, ker pozitiven odvod kaže gor, negativen pa dol.  mi hočemo najti minimum zato iščemo pot doll\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic = randomness in the process. only using one random data point at a time\n",
    "\n",
    "Gradient Descent = optimizing by moving in the direction of the steepest descent.\n",
    "\n",
    "SGD = calculates gradient using a single data point or small batch (not full dataset).\n",
    "\n",
    "Advantage = faster processing, less memory needed, helps avoid local minima.\n",
    "\n",
    "Drawback = noisier path, more fluctuation during training. a lot of dancing around the minima at the end\n",
    "\n",
    "\n",
    "this is usually the better solution compared to nonm stohadstic desvent.  because it nosiser,  the lower  the noise,  the more chanches it will be overfitt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_file_path = \"communities+and+crime/communities.data\"\n",
    "columns_to_remove = ['attr_0', 'attr_1', 'attr_2', 'attr_3', 'attr_4']  \n",
    "target_column = 'attr_127'  \n",
    "\n",
    "data = pd.read_csv(data_file_path, header=None, na_values='?')\n",
    "data.columns = [f'attr_{i}' for i in range(data.shape[1])]\n",
    "data = data.drop(columns=columns_to_remove)\n",
    "\n",
    "\n",
    "data = data.fillna(data.mean())\n",
    "\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method has high variance. Usually 10 fold split is best. the majority of data should be in the train set.  if 10% is too little you have a really small data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent with learning rate 0.01: RMSE = 0.13284996611813002\n",
      "Gradient Descent with learning rate 0.005: RMSE = 0.13557575821010873\n",
      "Gradient Descent with learning rate 0.001: RMSE = 0.15847398056483591\n",
      "Stochastic Gradient Descent with learning rate 0.01: RMSE = 0.13798850184393083\n",
      "Stochastic Gradient Descent with learning rate 0.005: RMSE = 0.13628631734229618\n",
      "Stochastic Gradient Descent with learning rate 0.001: RMSE = 0.13356126803518703\n"
     ]
    }
   ],
   "source": [
    "# Convert X and y to NumPy arrays\n",
    "X = X.values\n",
    "y = y.values.reshape(-1, 1)\n",
    "\n",
    "def ridge_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    for epoch in range(epochs):\n",
    "        predictions = X @ theta\n",
    "        errors = predictions - y\n",
    "        gradient = (1/m) * (X.T @ errors) + (alpha/m) * theta\n",
    "        theta -= learning_rate * gradient\n",
    "    return theta\n",
    "\n",
    "def ridge_stochastic_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(m):\n",
    "            xi = X[i:i+1]\n",
    "            yi = y[i:i+1]\n",
    "            prediction = xi @ theta\n",
    "            error = prediction - yi\n",
    "            gradient = xi.T @ error + (alpha/m) * theta\n",
    "            theta -= learning_rate * gradient\n",
    "    return theta\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "learning_rates = [0.01, 0.005, 0.001]\n",
    "alpha = 1.0\n",
    "epochs = 1000\n",
    "\n",
    "for lr in learning_rates:\n",
    "    theta_gd = ridge_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "    y_pred = X_test @ theta_gd\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "    print(f\"Gradient Descent with learning rate {lr}: RMSE = {rmse}\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    theta_sgd = ridge_stochastic_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "    y_pred = X_test @ theta_sgd\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "    print(f\"Stochastic Gradient Descent with learning rate {lr}: RMSE = {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are selecting the variable that is most colosely related to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the t stat.  knowing the probabiliy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13088\\1950730907.py:30: RuntimeWarning: overflow encountered in matmul\n",
      "  gradient = (1/m) * (X.T @ errors) + (alpha/m) * theta\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13088\\1950730907.py:30: RuntimeWarning: invalid value encountered in matmul\n",
      "  gradient = (1/m) * (X.T @ errors) + (alpha/m) * theta\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13088\\1950730907.py:31: RuntimeWarning: invalid value encountered in subtract\n",
      "  theta -= learning_rate * gradient\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m     theta_gd \u001b[38;5;241m=\u001b[39m ridge_gradient_descent(X_train, y_train, lr, alpha, epochs)\n\u001b[0;32m     57\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m X_test \u001b[38;5;241m@\u001b[39m theta_gd\n\u001b[1;32m---> 58\u001b[0m     rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradient Descent | Learning Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m gd_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_regression.py:506\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[0;32m    503\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[0;32m    504\u001b[0m         )\n\u001b[1;32m--> 506\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    510\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_regression.py:113\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[0;32m    111\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m    112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m--> 113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    116\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mreshape(y_true, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# 1. Load the White Wine Quality Dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "s = requests.get(url).content\n",
    "data = pd.read_csv(io.StringIO(s.decode('utf-8')), sep=';')\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "X = data.drop('quality', axis=1).values\n",
    "y = data['quality'].values.reshape(-1, 1)\n",
    "\n",
    "# 3. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Ridge Regression using Gradient Descent\n",
    "def ridge_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    for epoch in range(epochs):\n",
    "        predictions = X @ theta\n",
    "        errors = predictions - y\n",
    "        gradient = (1/m) * (X.T @ errors) + (alpha/m) * theta\n",
    "        theta -= learning_rate * gradient\n",
    "    return theta\n",
    "\n",
    "# 5. Ridge Regression using Stochastic Gradient Descent\n",
    "def ridge_stochastic_gradient_descent(X, y, learning_rate, alpha, epochs):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(m):\n",
    "            xi = X[i:i+1]\n",
    "            yi = y[i:i+1]\n",
    "            prediction = xi @ theta\n",
    "            error = prediction - yi\n",
    "            gradient = xi.T @ error + (alpha/m) * theta\n",
    "            theta -= learning_rate * gradient\n",
    "    return theta\n",
    "\n",
    "# 6. Test Different Learning Rates\n",
    "learning_rates = [0.01, 0.005, 0.001]\n",
    "alpha = 1.0\n",
    "epochs = 1000\n",
    "\n",
    "# Gradient Descent\n",
    "start_time = time.time()\n",
    "for lr in learning_rates:\n",
    "    theta_gd = ridge_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "    y_pred = X_test @ theta_gd\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"Gradient Descent | Learning Rate: {lr} | RMSE: {rmse}\")\n",
    "gd_time = time.time() - start_time\n",
    "print(f\"Gradient Descent Time: {gd_time} seconds\")\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "start_time = time.time()\n",
    "for lr in learning_rates:\n",
    "    theta_sgd = ridge_stochastic_gradient_descent(X_train, y_train, lr, alpha, epochs)\n",
    "    y_pred = X_test @ theta_sgd\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"Stochastic GD | Learning Rate: {lr} | RMSE: {rmse}\")\n",
    "sgd_time = time.time() - start_time\n",
    "print(f\"Stochastic GD Time: {sgd_time} seconds\")\n",
    "\n",
    "# 7. Scikit-learn Ridge and Lasso with GridSearchCV\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "\n",
    "param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "ridge_grid = GridSearchCV(ridge, param_grid, cv=5)\n",
    "ridge_grid.fit(X_train, y_train.ravel())\n",
    "print(f\"Best Ridge Alpha: {ridge_grid.best_params_['alpha']}\")\n",
    "\n",
    "lasso_grid = GridSearchCV(lasso, param_grid, cv=5)\n",
    "lasso_grid.fit(X_train, y_train.ravel())\n",
    "print(f\"Best Lasso Alpha: {lasso_grid.best_params_['alpha']}\")\n",
    "\n",
    "# 8. Compare Attributes Selected by Lasso\n",
    "lasso_best = lasso_grid.best_estimator_\n",
    "lasso_coefficients = pd.Series(lasso_best.coef_, index=data.columns[:-1])\n",
    "selected_features = lasso_coefficients[lasso_coefficients != 0].index.tolist()\n",
    "print(f\"Selected Features by Lasso: {selected_features}\")\n",
    "\n",
    "# Assume forward_selected_features from previous assignment\n",
    "forward_selected_features = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides']\n",
    "\n",
    "print(f\"Forward Selected Features: {forward_selected_features}\")\n",
    "print(f\"Common Features: {list(set(selected_features) & set(forward_selected_features))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
