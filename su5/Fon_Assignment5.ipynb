{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assigment we are building a regression tree. Boosting is like playing golf. First you hit quite hard. Then you try multiple times to come as near to the hole as possible. Don't make it too deep to avoid overfitting.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset on UË‡cilnica \"wine-quality.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('wine_quality.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['quality'] = data['quality'].map({'low': 0, 'high': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   quality  alcohol_level\n",
      "0        0              0\n",
      "1        0              0\n",
      "2        0              1\n",
      "3        0              0\n",
      "4        0              0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data['alcohol_level'] = data['alcohol_level'].map({'low': 0, 'high': 1})\n",
    "\n",
    "\n",
    "print(data[['quality', 'alcohol_level']].head())\n",
    "\n",
    "\n",
    "X = data.drop(['quality'], axis=1)\n",
    "y = data['quality']\n",
    "\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=42\n",
    ")  # 0.25 x 0.8 = 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code for gradient boosting of trees to solve a binary classification problem\n",
    "from scratch.Use the Scikit-learn regression tree implementation. The trees have to be weak learners,\n",
    "so their depth has to be set to a small value. You can choose their final depth arbitrar-\n",
    "ily. Think about overfitting.\n",
    "\n",
    ". Number of estimators is basically the number of boosting stages to be performed by the model\n",
    "\n",
    "The learning rate determines the step size at each iteration while moving toward a minimum of a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_boosting(X_train, y_train, X_val, y_val, learning_rate, n_estimators, max_depth):\n",
    "    \n",
    "    y_pred = np.full(y_train.shape, y_train.mean())\n",
    "    models = []\n",
    "    for _ in range(n_estimators):\n",
    "        residuals = y_train - y_pred\n",
    "        tree = DecisionTreeRegressor(max_depth=max_depth)\n",
    "        tree.fit(X_train, residuals)\n",
    "        update = tree.predict(X_train)\n",
    "        y_pred += learning_rate * update\n",
    "        models.append(tree)\n",
    "    \n",
    "    y_val_pred = np.full(y_val.shape, y_train.mean())\n",
    "    for tree in models:\n",
    "        y_val_pred += learning_rate * tree.predict(X_val)\n",
    "    y_val_pred_class = (y_val_pred >= 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred_class)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test different tree depths. The trees have to be weak learners.\n",
    "Test different learning rates. \n",
    "Test different numbers of trees that are built during gradient boosting and comment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.1, Max Depth: 1, N Estimators: 50, Accuracy: 0.7836734693877551\n",
      "Learning Rate: 0.1, Max Depth: 1, N Estimators: 100, Accuracy: 0.7908163265306123\n",
      "Learning Rate: 0.1, Max Depth: 1, N Estimators: 200, Accuracy: 0.8\n",
      "Learning Rate: 0.1, Max Depth: 2, N Estimators: 50, Accuracy: 0.8071428571428572\n",
      "Learning Rate: 0.1, Max Depth: 2, N Estimators: 100, Accuracy: 0.8163265306122449\n",
      "Learning Rate: 0.1, Max Depth: 2, N Estimators: 200, Accuracy: 0.8326530612244898\n",
      "Learning Rate: 0.1, Max Depth: 3, N Estimators: 50, Accuracy: 0.8214285714285714\n",
      "Learning Rate: 0.1, Max Depth: 3, N Estimators: 100, Accuracy: 0.8387755102040816\n",
      "Learning Rate: 0.1, Max Depth: 3, N Estimators: 200, Accuracy: 0.8469387755102041\n",
      "Learning Rate: 0.05, Max Depth: 1, N Estimators: 50, Accuracy: 0.7826530612244897\n",
      "Learning Rate: 0.05, Max Depth: 1, N Estimators: 100, Accuracy: 0.7836734693877551\n",
      "Learning Rate: 0.05, Max Depth: 1, N Estimators: 200, Accuracy: 0.7908163265306123\n",
      "Learning Rate: 0.05, Max Depth: 2, N Estimators: 50, Accuracy: 0.7928571428571428\n",
      "Learning Rate: 0.05, Max Depth: 2, N Estimators: 100, Accuracy: 0.8051020408163265\n",
      "Learning Rate: 0.05, Max Depth: 2, N Estimators: 200, Accuracy: 0.8163265306122449\n",
      "Learning Rate: 0.05, Max Depth: 3, N Estimators: 50, Accuracy: 0.8091836734693878\n",
      "Learning Rate: 0.05, Max Depth: 3, N Estimators: 100, Accuracy: 0.8204081632653061\n",
      "Learning Rate: 0.05, Max Depth: 3, N Estimators: 200, Accuracy: 0.8346938775510204\n",
      "Learning Rate: 0.01, Max Depth: 1, N Estimators: 50, Accuracy: 0.7826530612244897\n",
      "Learning Rate: 0.01, Max Depth: 1, N Estimators: 100, Accuracy: 0.7826530612244897\n",
      "Learning Rate: 0.01, Max Depth: 1, N Estimators: 200, Accuracy: 0.7826530612244897\n",
      "Learning Rate: 0.01, Max Depth: 2, N Estimators: 50, Accuracy: 0.7826530612244897\n",
      "Learning Rate: 0.01, Max Depth: 2, N Estimators: 100, Accuracy: 0.7826530612244897\n",
      "Learning Rate: 0.01, Max Depth: 2, N Estimators: 200, Accuracy: 0.7857142857142857\n",
      "Learning Rate: 0.01, Max Depth: 3, N Estimators: 50, Accuracy: 0.7826530612244897\n",
      "Learning Rate: 0.01, Max Depth: 3, N Estimators: 100, Accuracy: 0.7826530612244897\n",
      "Learning Rate: 0.01, Max Depth: 3, N Estimators: 200, Accuracy: 0.8020408163265306\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rates = [0.1, 0.05, 0.01]\n",
    "max_depths = [1, 2, 3]\n",
    "n_estimators_list = [50, 100, 200]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for depth in max_depths:\n",
    "        for n_estimators in n_estimators_list:\n",
    "            accuracy = gradient_boosting(X_train, y_train, X_val, y_val, lr, n_estimators, depth)\n",
    "            print(f'Learning Rate: {lr}, Max Depth: {depth}, N Estimators: {n_estimators}, Accuracy: {accuracy}')\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = {'learning_rate': lr, 'max_depth': depth, 'n_estimators': n_estimators}\n",
    "\n",
    "print('Best Parameters:', best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a good learning rate? A good learning rate is 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier Accuracy: 0.8520408163265306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    random_state=42\n",
    ")\n",
    "gbc.fit(X_train, y_train)\n",
    "gbc_pred = gbc.predict(X_val)\n",
    "gbc_accuracy = accuracy_score(y_val, gbc_pred)\n",
    "print('GradientBoostingClassifier Accuracy:', gbc_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Compare the results from your implementation with the \"GradientBoostingClassi-\n",
    "fier\" classifier implemented in Scikit-learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoostingClassifier Accuracy: 0.852\n",
    "\n",
    "Moja implementacija -> Accuracy: 0.847 Learning Rate: 0.1, Max Depth: 3, N Estimators: 200, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Regressor MSE: 0.10630660945269882\n",
      "XGBoost Regressor R2: 0.3750613331794739\n",
      "Best parameters found:  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n",
      "Lowest RMSE found:  0.34236755904118854\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "xgb_reg = xgb.XGBRegressor(\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42\n",
    ")\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "xgb_pred = xgb_reg.predict(X_val)\n",
    "xgb_mse = mean_squared_error(y_val, xgb_pred)\n",
    "xgb_r2 = r2_score(y_val, xgb_pred)\n",
    "print('XGBoost Regressor MSE:', xgb_mse)\n",
    "print('XGBoost Regressor R2:', xgb_r2)\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'n_estimators': [50, 100, 200,500]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Lowest RMSE found: \", (-grid_search.best_score_)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try modeling your data also with XGBoost and try to find the best set of hyperpa-\n",
    "rameters.\n",
    "When you perform hyperparameter tuning be careful to use a validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Best parameters found:  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
